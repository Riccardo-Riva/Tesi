  LCM-FARMHELP-IT(7)     Miscellaneous Information Manual     LCM-FARMHELP-IT(7)

NOME
       lcm-farmhelp-it  -  guida  introduttiva  all'utilizzo  della  farm com‐
       putazionale LCM

DESCRIZIONE
       La farm computazionale LCM è un cluster ibrido Debian/CentOS  a  dispo‐
       sizione  del  gruppo di ricerca di fisica teorica dell'Università degli
       Studi di Milano per l'esecuzione di simulazioni e altre necessità  com‐
       putazionali.   L'infrastruttura informatica è gestita dagli amministra‐
       tori di sistema del Laboratorio di Calcolo e Multimedia (LCM) sotto  la
       supervisione del Prof. Alessandro Vicini.

   
       In  questa  guida  si  farà  riferimento  al cluster del gruppo teorico
       tramite la locuzione "farm computazionale" e al cluster utilizzabile da
       parte di tutti gli utenti LCM con la locuzione "cluster LCM".

   LANCIARE UN JOB
       Oltre  che dal server galileo su cui vi trovate, la farm computazionale
       è costituita da una  serie  di  nodi  computazionali  cui  l'accesso  è
       precluso  all'utenza.   È  SEVERAMENTE VIETATO lanciare job su galileo;
       ogni job deve invece essere inserito tramite il gestore di code Slurm.

       Il comando per inserire i job in coda è sbatch.  Un esempio  elementare
       per il lancio di un eseguibile myscript.x è il seguente:

              sbatch     --cpus-per-task=1    --mem-per-cpu=1G    --time=2-0:0
              --wrap="/percorso/di/myscript.x"

       con il quale si richiede a Slurm di riservare un processore e 1  GB  di
       RAM  per un tempo limite di 2 giorni per eseguire il job indicato.  Una
       corretta stima del tempo di esecuzione è cruciale:  un  job  con  tempo
       limite inferiore avrà maggiore probabilità di essere eseguito prima, ma
       una volta decorso il tempo stabilito il processo sarà interrotto.

       Il comando sbatch accetta anche input da file. La riga di cui  sopra  è
       equivalente a

              sbatch mysbatchscript.sh

       se il file mysbatchscript.sh contiene:

              #!/bin/bash
              #SBATCH --cpus-per-task=1
              #SBATCH --mem-per-cpu=1G
              #SBATCH --time=2-0:0

              /percorso/di/myscript.x

   MONITORARE E CANCELLARE UN JOB
       Una  lista dei job in coda è accessibile tramite il comando squeue.  Il
       comando

              scontrol show job JOBID

       fornisce informazioni più  dettagliate  sul  job  JOBID.   È  possibile
       rimuovere un proprio job in coda tramite il comando

              scancel JOBID

   
   SPAZIO PERSONALE
       Le  cartelle /home di galileo sono distinte da quelle del cluster LCM e
       non sono soggette a quota; essendo tuttavia lo spazio  su  disco  limi‐
       tato,  si  raccomanda  di mantenerle entro 50 GB per garantire un'espe‐
       rienza ottimale a tutti gli utenti.

       È possibile richiedere una cartella personale in  /farmstorage  qualora
       fosse  necessario  ulteriore  spazio di archiviazione.  Per progetti di
       gruppo si può inoltre richiedere una cartella condivisa  in  /projects,
       soggetta  a quota di 80 GB superabile per 3 giorni fino a un massimo di
       100 GB.

       I file system /home, /farmstorage e /projects  sono  tutti  e  tre  es‐
       portati  sui  nodi computazionali e sono dunque accessibili a qualsiasi
       job. Le /home del cluster LCM sono  esportate  in  /lcm/home  sul  solo
       galileo.

       Ogni  job  inserito  tramite  Slurm  dispone di una cartella locale sul
       disco del nodo il cui percorso è salvato nella  variabile  di  ambiente
       SLURM_TMPDIR.   Tale  cartella  è cancellata permanentemente al termine
       dell'esecuzione del job: pertanto, il modo corretto  di  utilizzarla  è
       copiare  i  file richiesti dalla propria /home (o da /projects e /farm‐
       storage) a SLURM_TMPDIR all'inizio dell'esecuzione del job, e copiare i
       risultati  nella cartella desiderata alla fine del job.  Qualora un job
       necessitasse di effettuare molta lettura o scrittura su disco, è OBBLI‐
       GATORIO usare tale cartella per i file temporanei al fine di evitare un
       sovraccarico dei tre file system distribuiti.

   CODE DISPONIBILI
       Slurm permette di selezionare la coda in cui inserire  il  proprio  job
       tramite l'opzione --partition del comando sbatch:

              sbatch --partition=PARTITION ...

       Le code attualmente disponibili sono le seguenti:

              •  general: coda di utilizzo generale.  Qualora nelle opzioni di
              sbatch non sia specificata alcuna preferenza di  partizione,  il
              job sarà assegnato in automatico a questa.

              • multicore: coda riservata a job che richiedono almeno 20 core.
              Questa coda è in fase di test: in futuro potrà cambiare  il  nu‐
              mero  di nodi disponibili e il minimo di core richiesti per lan‐
              ciare un job.

              • softload: coda che include nodi dalle specifiche hardware  ri‐
              dotte  rispetto  a quelli facenti parte della partizione di uti‐
              lizzo generale.  È consigliato farne uso per job  poco  esigenti
              dal punto di vista computazionale.

VEDERE ANCHE
       slurm(1), sbatch(1), squeue(1), scontrol(1), scancel(1)

                                  2019-11-13                LCM-FARMHELP-IT(7)


srun --pty 